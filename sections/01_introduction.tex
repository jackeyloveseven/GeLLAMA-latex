\section{Introduction}

The ability to control the output of generative models is a cornerstone of modern content creation. While diffusion models have demonstrated remarkable capabilities in synthesizing photorealistic images from text, achieving fine-grained control, particularly for artistic and design purposes, remains a significant challenge. A key limitation of many state-of-the-art style transfer methods is the phenomenon of \textit{style-content entanglement} \cite{yang2024multiparty}, where the model struggles to disentangle the artistic style from the semantic content of a reference image. This often leads to \textit{content leakage}, where structural elements of the style reference undesirably manifest in the generated output \cite{wang2024instantstyle, xing2024csgo}.

This entanglement problem becomes particularly acute in \textit{local stylization}, a task that demands the application of a style to a precisely delineated region of a target image. In this context, the shortcomings of existing approaches are magnified, leading to a cascade of artifacts. These include \textbf{style bleeding}, where the style's influence spills beyond the intended masked area; \textbf{edge discrepancies}, which create visually jarring seams; and a general \textbf{lack of physical realism}, where the synthesized texture fails to conform to the underlying geometry and lighting of the scene.

We posit that these issues originate from a fundamental limitation: the lack of an explicit, physically-grounded guidance mechanism. Current methods often rely on the model to implicitly learn the complex interplay between style, content, geometry, and lighting. Inspired by the success of energy-guided frameworks like FreeDoM \cite{shi2023freedom} in providing flexible, training-free control, we propose a new approach that offers a more direct and spatially precise form of guidance.

We introduce \textbf{}, a novel framework for A \textbf{Ge}ometry and \textbf{L}ighting-\textbf{a}ware Framework for Photorealistic \textbf{L}ocal Stylization via \textbf{M}asked \textbf{A}ttention Modulation. Unlike global energy-based methods, GeLLAM is designed specifically for tasks requiring exquisite spatial control. Our framework is built on a foundation of physical priors, explicitly conditioning the generation process on high-fidelity depth maps and simulated lighting environments. The core of our contribution is the \textbf{MaskedStyleAttnProcessor}, a versatile attention-level mechanism that uses a mask to perform a smooth, semantic fusion of style and content features directly within the U-Net. This allows for a seamless integration at object boundaries, a feat that is difficult to achieve with global guidance alone.

Our contributions are as follows:
\begin{itemize}
    \item We propose GeLLAM, a framework that significantly enhances the photorealism of local stylization by explicitly integrating geometric and lighting priors into the generative process.
    \item We design the \textbf{MaskedStyleAttnProcessor}, a novel and flexible attention mechanism that provides precise spatial control over feature fusion, effectively resolving style-content entanglement at a local level and eliminating boundary artifacts.
    \item We demonstrate the versatility of our approach, showing its superior performance not only in high-fidelity local stylization but also in preventing content leakage in global style transfer tasks, outperforming existing state-of-the-art methods.
\end{itemize}