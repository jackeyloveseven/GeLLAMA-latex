\section{Conclusion}

In this paper, we introduced GMSI-Net, a novel framework designed to address the critical challenges of achieving photorealistic local stylization. By systematically incorporating geometric and lighting priors, our method provides a robust foundation for the diffusion model, guiding it towards physically plausible and visually coherent results. The cornerstone of our work, the \texttt{MaskedStyleAttnProcessor}, offers a sophisticated solution to the pervasive problems of edge artifacts and style leakage. By modulating style injection at the feature level, it achieves a seamless semantic fusion that is superior to traditional pixel-based blending techniques.

Furthermore, we have demonstrated that the principles underlying our attention mechanism extend effectively to global style transfer. Our method excels at separating artistic style from semantic content, preventing the content leakage that plagues many state-of-the-art models. This dual applicability makes our approach a versatile and powerful tool for a wide range of creative image editing tasks.

Future work could explore several exciting directions. Extending the GMSI-Net framework to video stylization is a natural next step, which would require ensuring temporal consistency across frames. Additionally, while our model accounts for geometry and lighting, incorporating more complex material properties, such as specularity and subsurface scattering, could push the boundaries of realism even further. We believe that our work provides a solid foundation for future research in physically-grounded, controllable, and high-fidelity generative art.