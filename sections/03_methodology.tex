\section{Methodology}

Our GMSI-Net framework is designed to achieve photorealistic local stylization by explicitly integrating geometric and lighting priors into the diffusion process, and by employing a novel masked attention mechanism for precise spatial control. This section details the architecture and key components of our system.

\subsection{Framework Overview}

The overall pipeline of GMSI-Net is illustrated in Figure~\ref{fig:framework}. Given a content image $I_c \in \mathbb{R}^{H \times W \times 3}$, a style image $I_s \in \mathbb{R}^{H_s \times W_s \times 3}$, and a binary mask $M \in \{0, 1\}^{H \times W}$ defining the target stylization region, our framework operates in three main stages:
\begin{enumerate}
    \item \textbf{Geometric Prior Extraction:} We first extract a high-fidelity depth map $D \in \mathbb{R}^{H \times W}$ from the content image $I_c$. This depth map serves as a crucial geometric prior, providing the diffusion model with accurate 3D structural information.
    \item \textbf{Lighting-Aware Initialization:} Using the extracted depth map $D$, we synthesize a shaded `init_image` $I_{init} \in \mathbb{R}^{H \times W \times 3}$. This image incorporates a plausible lighting environment, offering a physically grounded starting point for the subsequent diffusion process.
    \item \textbf{Masked Style Injection:} The core of our framework involves a diffusion model (e.g., SDXL) conditioned by $I_{init}$, $I_s$, and $D$ (via ControlNet). Crucially, our custom \texttt{MaskedStyleAttnProcessor} is integrated into the U-Net's cross-attention layers. This processor leverages the input mask $M$ to precisely modulate the influence of style features, ensuring they are confined to the target region and seamlessly blended at the boundaries, thereby preventing style-content entanglement.
\end{enumerate}

\begin{figure}[h]
  \centering
  \fbox{Framework Diagram Placeholder}
  \caption{The overall architecture of GMSI-Net. The process begins with extracting geometry and lighting priors from the content image. These priors, along with the style image, guide the diffusion model. Our core contribution, the \texttt{MaskedStyleAttnProcessor}, modulates the style injection at the attention level using the input mask, leading to a photorealistic and seamlessly integrated result.}\n  \label{fig:framework}
\end{figure}

\subsection{Geometric Prior Extraction}

Accurate geometric understanding is paramount for generating textures that conform realistically to object surfaces. We employ a two-stage process to obtain a high-fidelity depth map $D$. First, we utilize \textbf{DepthAnythingV2} \cite{li2024depthanythingv2}, a state-of-the-art monocular depth estimation model, to generate an initial depth map. To enhance fine-grained details and ensure the depth map captures subtle surface variations crucial for texture rendering, we apply a \textbf{MultiScaleDepthEnhancement} module. This module refines the initial depth map, producing $D$ with improved sharpness and structural integrity.

\subsection{Lighting-Aware Initialization}

To provide the diffusion model with a physically consistent starting point, we generate a lighting-aware `init_image` $I_{init}$. Our \textbf{DirectionalShadingModule} takes the enhanced depth map $D$ as input and simulates a simple directional lighting environment. It computes surface normals from $D$ and then calculates diffuse shading based on a predefined light source direction. This process generates $I_{init}$ that inherently contains realistic shadows and highlights, reflecting the object's 3D form under a specific illumination. By initializing the diffusion process with $I_{init}$ instead of a simple masked image or pure noise, we significantly reduce the model's burden to infer plausible lighting, thereby encouraging the generated style to interact with the scene's illumination in a physically believable manner.

\subsection{Masked Attention for Style Modulation}

The central innovation of our framework is the \texttt{MaskedStyleAttnProcessor}, which enables precise spatial control over style feature injection within the U-Net architecture of the diffusion model. This mechanism is crucial for resolving style-content entanglement and ensuring seamless transitions at mask boundaries.

\subsubsection{Background: Cross-Attention in Diffusion Models}
In diffusion models, style features (e.g., from an IP-Adapter) are typically injected into the U-Net via cross-attention layers. For a given query $Q$ (from the content/noise image features), key $K$ and value $V$ (from the style image features), the attention output $A_{style}$ is computed as:
\begin{equation}
    A_{style} = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \label{eq:standard_attention}
\end{equation}
where $d_k$ is the dimension of the key vectors. In this standard formulation, the style features in $K$ and $V$ have a global influence across the entire feature map, which can lead to unintended style bleeding or content leakage.

\subsubsection{The MaskedStyleAttnProcessor}
Our processor modifies this standard cross-attention mechanism by incorporating spatial guidance from the input mask $M$. The mask $M$ is first down-sampled to match the spatial dimensions of the feature map at each U-Net layer, denoted as $M_{feat} \in \{0, 1\}^{h \times w}$, where $h \times w$ are the dimensions of the feature map. The core idea is to blend the style-conditioned attention output with the content-preserving self-attention output, using $M_{feat}$ as a spatial weighting factor.

Let $A_{cross}$ be the output of the cross-attention layer (as in Equation \ref{eq:standard_attention}) and $A_{self}$ be the output of the self-attention layer (which primarily preserves content information). Our \texttt{MaskedStyleAttnProcessor} computes a fused attention output $A_{fused}$ as follows:
\begin{equation}
    A_{fused} = M_{feat} \odot A_{cross} + (1 - M_{feat}) \odot A_{self}
    \label{eq:fused_attention}
\end{equation}
where $\odot$ denotes element-wise multiplication. This operation ensures that in regions corresponding to the mask ($M_{feat}=1$), the style-conditioned features dominate, while outside the mask ($M_{feat}=0$), the original content features are preserved. Crucially, at the boundaries of the mask, where $M_{feat}$ transitions from 0 to 1, the blending occurs in the high-dimensional feature space. This enables a smooth, semantic interpolation that respects local image semantics (e.g., light, shadow, texture gradients), thereby preventing harsh edges and unnatural artifacts that arise from simple pixel-space alpha blending. This mechanism effectively disentangles style and content at a fine-grained spatial level, allowing for precise and photorealistic local stylization.