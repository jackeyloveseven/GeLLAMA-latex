\section{Experiments}

We conduct a comprehensive set of experiments to validate the effectiveness of GMSI-Net. We evaluate our framework on two primary fronts: (1) the quality and realism of local stylization, and (2) the purity of style in global style transfer. We perform qualitative comparisons against state-of-the-art methods and conduct ablation studies to demonstrate the contribution of each component in our framework.

\subsection{Implementation Details}
Our framework is built upon a Stable Diffusion XL (SDXL) base model. For geometric prior extraction, we use the publicly available DepthAnythingV2 model. The IP-Adapter is used for style feature extraction. All experiments are conducted on a single NVIDIA RTX 4090 GPU. We use a standard set of hyperparameters for the diffusion process, with 50 denoising steps and a guidance scale of 7.5.

\subsection{Local Stylization Evaluation}

In this section, we assess GMSI-Net's ability to perform high-fidelity local stylization, focusing on edge quality, style containment, and detail preservation.

\subsubsection{Qualitative Comparison}
We compare our method against two primary baselines: (1) Standard SDXL Inpainting combined with a global IP-Adapter, and (2) The same setup but with a simple Gaussian blur applied to the mask edge (feathering) to attempt a smoother blend. As shown in Figure~\ref{fig:local_comparison}, the standard method suffers from significant style leakage and produces harsh, unrealistic edges. While feathering the mask slightly mitigates the hard edge, it introduces a blurry, washed-out transition zone. In contrast, our GMSI-Net produces a sharp, clean, and semantically coherent boundary that naturally integrates the new texture with the original image, preserving lighting details across the edge.

\begin{figure}[h]
  \centering
  \fbox{Local Stylization Comparison Placeholder}
  \caption{Qualitative comparison for local stylization. From left to right: Content Image + Mask, Baseline 1 (Standard Inpainting), Baseline 2 (Feathered Mask), Our GMSI-Net. Our method demonstrates superior edge integration and prevents style leakage into the background.}
  \label{fig:local_comparison}
\end{figure}

\subsubsection{Ablation Studies}
To validate the importance of each component in our framework, we conduct a series of ablation studies, the results of which are shown in Figure~\ref{fig:ablation}. We test three variants of our model:
\begin{itemize}
    \item \textbf{w/o Geometry:} The model operates without the depth map from DepthAnythingV2. The resulting texture fails to conform to the object's underlying surface, appearing flat and painted-on.
    \item \textbf{w/o Lighting:} The model starts from a simple masked image instead of our shaded `init_image`. The generated texture, while correctly placed, lacks realistic interaction with the scene's lighting, resulting in flat-looking materials with poor shadow and highlight representation.
    \item \textbf{w/o Masked Attention:} We use the full GMSI-Net pipeline but replace our \texttt{MaskedStyleAttnProcessor} with a standard cross-attention mechanism. This results in a regression to the baseline, with noticeable style leakage and a harsh boundary, demonstrating the critical role of our attention modulation in achieving seamless fusion.
\end{itemize}

\begin{figure}[h]
  \centering
  \fbox{Ablation Study Placeholder}
  \caption{Ablation studies demonstrating the contribution of each component. From left to right: Full GMSI-Net, w/o Geometry, w/o Lighting, w/o Masked Attention. The results confirm that all three components are essential for achieving photorealistic results.}
  \label{fig:ablation}
\end{figure}

\subsection{Global Style Transfer Evaluation}

We now evaluate the core attention mechanism of our framework on the task of global style transfer, specifically focusing on its ability to prevent content leakage from the style image.

\subsubsection{Comparison with State-of-the-Art}
We compare our method against three recent, powerful style transfer models: InstantStyle \cite{wang2024instantstyle}, CSGO \cite{xing2024csgo}, and StyleShot \cite{gao2024styleshot}. For a fair comparison, we apply our attention mechanism globally (i.e., with a mask of all ones). The key experiment, shown in Figure~\ref{fig:global_comparison}, uses a portrait as the content image and a painting of the Eiffel Tower as the style reference.

The results are striking. The baseline methods, while successfully transferring the overall color palette and texture, also erroneously incorporate structural artifacts from the Eiffel Tower into the generated portrait, a clear case of content leakage. Our method, however, successfully isolates the artistic *style*—the brushstrokes, the color tones—and applies it to the portrait without transferring any of the tower's structure. This demonstrates a superior disentanglement of content and style, enabled by our attention mechanism's design.

\begin{figure}[h]
  \centering
  \fbox{Global Style Transfer Comparison Placeholder}
  \caption{Comparison of global style transfer methods to highlight content leakage. From left to right: Content Image, Style Image (Eiffel Tower), InstantStyle, CSGO, StyleShot, Ours. Our method is the only one that successfully transfers the style without leaking content from the style reference.}
  \label{fig:global_comparison}
\end{figure}

Overall, our method provides a higher-purity stylization, preserving the content of the target image while faithfully applying the artistic characteristics of the style reference.
This makes it a more reliable tool for artists and designers who require precise control over the creative process.