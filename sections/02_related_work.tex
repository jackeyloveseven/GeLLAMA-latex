\section{Related Work}

Our research is situated at the intersection of several active areas in generative modeling: training-free conditional generation, controllable style transfer, and attention mechanism manipulation.

\subsection{Training-Free Conditional Generation}
A significant paradigm in controllable diffusion models is the training-free approach, which avoids the high costs and inflexibility of retraining models for new conditions. A prominent strategy in this domain is energy-guided generation. Works like FreeDoM \cite{shi2023freedom} have demonstrated remarkable flexibility by using off-the-shelf, pre-trained networks as energy functions to guide the sampling process towards a desired condition. This allows for a wide range of controls, from text and face IDs to style images, without any model finetuning. However, these methods typically apply guidance globally across the entire image. While effective for general-purpose tasks, this global application lacks the precision required for fine-grained local editing, where controlling the spatial extent of an edit is paramount. Our work is inspired by this energy-guided philosophy but adapts it to a spatially-aware context, offering a mechanism for localized, rather than global, control.

\subsection{Controllable Style Transfer}
Style transfer has been a benchmark task for generative models. Recent methods have focused on improving style fidelity and content preservation. InstantStyle \cite{wang2024instantstyle} and StyleShot \cite{gao2024styleshot} introduce lightweight, tuning-free mechanisms for style injection. CSGO \cite{xing2024csgo} and DEADiff \cite{qi2024deadiff} focus on explicitly disentangling content and style representations. A common challenge faced by these methods is \textit{content leakage}, where the model inadvertently transfers semantic content from the style reference. Our work addresses this issue from a new perspective: by providing precise spatial control over where style features are applied, we can fundamentally prevent the style from influencing regions where it would conflict with the content, thus avoiding leakage.

\subsection{Manipulation of Attention Mechanisms}
The cross-attention layers in diffusion models are a primary interface for injecting conditioning information, making them a focal point for manipulation. Prompt-to-Prompt \cite{hertz2022prompt} pioneered the editing of attention maps to control image synthesis. Subsequent works like MCA-Ctrl \cite{yang2024multiparty} have refined this by proposing collaborative attention control to mitigate \textit{concept entanglement} in multi-concept generation. These methods prove that direct intervention in the attention mechanism is a powerful tool for semantic control. Our \texttt{MaskedStyleAttnProcessor} builds on this insight. While previous works focused on disentangling discrete semantic concepts (e.g., a cat and a dog), our processor is designed to handle the more amorphous and continuous nature of artistic style. It uses a mask to perform a soft, feature-space interpolation between styled and unstyled regions, a mechanism specifically tailored for the seamless visual integration required by photorealistic stylization.

\subsection{Geometry and 3D-Aware Synthesis}
There is a clear and growing trend towards integrating 3D awareness into 2D generative models to enhance realism. ZeST \cite{cheng2024zest} and Material Palette \cite{lopes2024material} leverage geometric information for material transfer and extraction. ControlNet \cite{zhang2023adding} famously uses depth maps, among other conditions, to control the structure of generated images. Our GMSI-Net contributes to this direction by not only using a high-fidelity depth prior but also by being the first, to our knowledge, to combine it with a physically-based lighting simulation as an explicit `init_image` for local stylization. This holistic consideration of both geometry and lighting provides a more robust foundation for generating textures that are not just plausible, but physically integrated into the scene.
